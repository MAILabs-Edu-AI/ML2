
## Алгоритмы

Логистическая регрессия реализована самым стандартным способом без регуляризаций.

KNN так же вполне стандартен, и реализован с использованием обычной Евклидовой нормы.

При реализации решающего используется критерий разбиения Джини, построение дерева заканчивается при достижении максимальной глубины.

Алгоритм случайного леса реализован на основе алгоритма решающего дерева. При этом используется беггинг и метод случайных подпространств.

## Метрики
Для оценки качества классификации использованы метрика accuracy, так как в обеих задачах отсутсвует дисбаланс таргета. Валидирование проводилось посредством кросс-валидации.

##  Результаты

Алгоритмы логистической регрессии, метода ближайших соседей  и решающее дерево в моей и библиотечной реализации показали сравнимые результаты, однако мои версии оказались куда более медленными. Это обусловленно однопоточной реализацией, и отсутстивем отпимизаций программы. Если с результатами линейной модели и KNN еще можно смириться, то с деревом дела обстоят куда хуже.

Библиотечная реализация случайного леса превосходит самодельную по качеству на обоих датасетах.

На обоих датасетах при выбранных параметрах алгоритмов видно, что модели не переобучились т.к. разница в точности классификации на обучающей и тестовой выборках ничтожно мала.

# Возникшие проблемы
Основные трудности, как и полагается, состояли в понимании алгоритма и последующей его реализации.
Из представленных в ноутбуках резлуьтатов заметно, что некоторые реализованные алгоритмы себя показали намного медленне библиотечных версий, что также связано с некоторыми костылями в коде.

# Выводы

В результате работы над реализацией алгоритмов машинного обучения стало понятно, что делать это стоит исключительно в образовательных целях. Не стоит "изобретать велосипед", когда существуют общедоступные реализации алгоритмов, значительно превосходящие самодельные.